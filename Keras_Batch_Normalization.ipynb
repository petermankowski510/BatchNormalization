{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Keras\n",
    "Normalize each batch by both mean and variance reference\n",
    "### Purpose\n",
    "As the data flows through a deep network, the weights and parameters adjust those values, sometimes making the data too big or too small again - a problem the authors refer to as \"internal covariate shift\". By normalizing the data in each mini-batch, this problem is largely avoided.\n",
    "\n",
    "### Benefits:\n",
    "- Networks train faster converge much more quickly,\n",
    "- Allows higher learning rates Gradient descent usually requires small learning rates for the network to converge.\n",
    "- Makes weights easier to initialize\n",
    "- Makes more activation functions viable Because batch normalization regulates the values going into each activation function, non-linearlities that don't seem to work well in deep networks actually become viable again.\n",
    "- May give better results overall  it's really an optimization to help train faster, so you shouldn't think of it as a way to make your network better.\n",
    "\n",
    "### Keypoints\n",
    "- Batch normalization uses weights as usual, but does NOT add a bias term. This is because, its calculations include gamma and beta variables that make the bias term unnecessary. In Keras `Dense(64, use_bias=False)` or `Conv2D(32, (3, 3), use_bias=False)`\n",
    "- We add the normalization before calling the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)  # one-hot\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(x, y, batch_size=32):\n",
    "    batches = int(len(x)/batch_size)\n",
    "    while 1:\n",
    "        for i in range(batches):\n",
    "            yield x[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=(28, 28, 1)\n",
    "class NeuralNet:\n",
    "    def __init__(self, use_batch_norm, activation='relu'):\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.build_model(activation=activation)\n",
    "    def add_dense_layer(self, units, activation='relu'):\n",
    "        if self.use_batch_norm:\n",
    "            self.model.add(layers.Dense(units, use_bias=False))\n",
    "            self.model.add(layers.BatchNormalization())\n",
    "            self.model.add(layers.Activation(activation))\n",
    "        else:\n",
    "            self.model.add(layers.Dense(units, activation=activation))\n",
    "    def add_conv2d_layer(self, filters, kernel_size, activation='relu', **kwargs):\n",
    "        if self.use_batch_norm:\n",
    "            self.model.add(layers.Conv2D(filters, kernel_size, use_bias=False, **kwargs))\n",
    "            self.model.add(layers.BatchNormalization())\n",
    "            self.model.add(layers.Activation(activation))\n",
    "        else:\n",
    "            self.model.add(layers.Conv2D(filters, kernel_size, activation=activation, **kwargs))\n",
    "    def build_model(self, activation):\n",
    "        self.model = models.Sequential()\n",
    "        self.add_conv2d_layer(32, (3, 3), activation=activation, input_shape=img_size)\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.add_conv2d_layer(64, (3, 3), activation=activation)\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.add_conv2d_layer(64, (3, 3), activation=activation)\n",
    "        self.model.add(layers.Flatten())\n",
    "        self.add_dense_layer(64, activation=activation)\n",
    "        self.add_dense_layer(10, activation='softmax')\n",
    "    def train(self, learning_rate=0.001, epoches=40, batch_size=32, steps_per_epoch=30):\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "        history = self.model.fit_generator(generator=data_generator(train_images, train_labels, batch_size=batch_size),\n",
    "                                      steps_per_epoch = steps_per_epoch,\n",
    "                                      epochs = epoches,\n",
    "                                      validation_data=data_generator(test_images, test_labels, batch_size=batch_size),\n",
    "                                      validation_steps=len(test_images)/batch_size)\n",
    "        return history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_accuracies(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Displays a plot of the accuracies calculated during training to demonstrate\n",
    "    how many iterations it took for the model(s) to converge.\n",
    "    \n",
    "    :param args: One or more NeuralNet objects\n",
    "        You can supply any number of NeuralNet objects as unnamed arguments \n",
    "        and this will display their training accuracies. Be sure to call `train` \n",
    "        the NeuralNets before calling this function.\n",
    "    :param kwargs: \n",
    "        You can supply any named parameters here, but `steps_per_epoch` is the only\n",
    "        one we look for. It should match the `steps_per_epoch` value you passed\n",
    "        to the `train` function.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    steps_per_epoch = kwargs['steps_per_epoch']\n",
    "    \n",
    "    for history in args:\n",
    "        ax.plot(range(0,len(history['acc'])*steps_per_epoch,steps_per_epoch),\n",
    "                history['acc'], label=\"acc-\" + history['name'])\n",
    "        ax.plot(range(0,len(history['val_acc'])*steps_per_epoch,steps_per_epoch),\n",
    "                history['val_acc'], label=\"val_acc-\"+history['name'])\n",
    "    ax.set_xlabel('Training steps')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy During Training')\n",
    "    ax.legend(loc=4)\n",
    "    #ax.set_ylim([0,1])\n",
    "    #plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def train_and_test(learning_rate=0.001, activation=\"relu\", epochs=40, steps_per_epoch=30):\n",
    "    nn = NeuralNet(use_batch_norm=False, activation=activation)\n",
    "    bn = NeuralNet(use_batch_norm=True, activation=activation)\n",
    "    history_nn = nn.train(learning_rate=learning_rate, epoches=epochs, steps_per_epoch=steps_per_epoch)\n",
    "    history_bn = bn.train(learning_rate=learning_rate, epoches=epochs, steps_per_epoch=steps_per_epoch)\n",
    "    history_nn['name'] = \"Without batch normalization\"\n",
    "    history_bn['name'] = \"With batch normalization\"\n",
    "    plot_training_accuracies(history_nn, history_bn, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_test(learning_rate=0.001, activation='relu', epochs=3, steps_per_epoch=1875)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a sigmoid activation function takes a long time to start learning. It eventually starts making progress, but it took over 250 batches just to get over 70% accuracy. Using batch normalization gets to 90% in around 100 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(learning_rate=0.001, activation='sigmoid', epochs=3, steps_per_epoch=1875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(learning_rate=0.01, activation='relu', epochs=3, steps_per_epoch=1875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(learning_rate=0.01, activation='sigmoid', epochs=3, steps_per_epoch=1875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(learning_rate=0.05, activation='relu', epochs=3, steps_per_epoch=1875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(learning_rate=0.05, activation='sigmoid', epochs=3, steps_per_epoch=1875)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a smaller steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(learning_rate=0.001, activation='relu', epochs=10, steps_per_epoch=int(1875/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
